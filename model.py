# -*- coding: utf-8 -*-
"""Model.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1cFtl4aJ1xWVRe93EpcVDRbQBcEJEgc7D
"""

import torch
import torch.nn as nn
#Model

device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
#device='cpu'
print(device)

class bert_span(nn.Module):
    def __init__(self,):
        super(bert_span,self).__init__()

        self.word_lstm = nn.LSTM(768,256,batch_first=True, bidirectional=True)

        self.q_w = nn.Linear(512,512)
        self.k_w = nn.Linear(512,512)
        self.v_w = nn.Linear(512,512)

        self.sent_lstm = nn.LSTM(512,256,batch_first = True, bidirectional = True)

        #self.conv = nn.Conv1d(768,512,4,stride =4)
        #self.maxpool = nn.MaxPool

        self.q_s = nn.Linear(512,512)
        self.k_s = nn.Linear(512,512)
        self.v_s = nn.Linear(512,512)

        dropout = 0.5

        self.softmax = nn.Softmax(dim=1)
        self.sigmoid = nn.Sigmoid()
        self.relu = nn.Tanh()
        self.dropout = nn.Dropout(dropout)

        #emotion

        self.emo_fc1 = nn.Linear(768,512)
        self.emo_fc2 = nn.Linear(512,256)
        self.emo_out = nn.Linear(256,7)

        #sentiment

        self.sen_fc1 = nn.Linear(768,512)
        self.sen_fc2 = nn.Linear(512,256)
        self.sen_out = nn.Linear(256,7)

        self.comp_fc1 = nn.Linear(768,512)
        self.comp_fc2 = nn.Linear(512,256)
        self.comp_fc3 = nn.Linear(256,128)
        self.comp_out = nn.Linear(128,2)

        self.span_fc1 = nn.Linear(768,512)
        self.span_fc2 = nn.Linear(256,128)

        #self.comp_fc = nn.Linear(128,64)
        self.span_out = nn.Linear(128,20)

        self.sev_out = nn.Linear(128,5)

        #attention

        self.alphas_m1 = nn.ParameterList([torch.nn.Parameter(torch.rand(1)) for i in range(1)])
        self.alphas_m2 = nn.ParameterList([torch.nn.Parameter(torch.rand(1)) for i in range(1)])



        self.alphas_e = nn.ParameterList([torch.nn.Parameter(torch.rand(1)) for i in range(2)])
        self.alphas_s = nn.ParameterList([torch.nn.Parameter(torch.rand(1)) for i in range(2)])
        self.alphas_c = nn.ParameterList([torch.nn.Parameter(torch.rand(1)) for i in range(2)])
        self.alphas_sp = nn.ParameterList([torch.nn.Parameter(torch.rand(1)) for i in range(2)])

        #self.comp_out = nn.Linear(84,2)


    def forward(self,x):

        batch = x.shape[0]

        word_lstm,_ = self.word_lstm(x)

        q_w = self.q_w(word_lstm)
        k_w = self.k_w(word_lstm).permute(0,2,1)
        v_w = self.v_w(word_lstm)

        soft_att = self.softmax(torch.bmm(q_w,k_w))

        self_w_att = torch.bmm(soft_att,v_w)


        self_w_att = self_w_att.reshape(batch,-1,4,512)

        self_w_att = torch.mean(self_w_att,2)
        #self_w_att = torch.mean(word_lstm,1)

        sent_lstm,_ = self.sent_lstm(self.dropout(self_w_att))
        #sent_lstm,_ = self.sent_lstm(x)

        q_s = self.q_s(sent_lstm)
        k_s = self.k_s(sent_lstm).permute(0,2,1)
        v_s = self.v_s(sent_lstm)

        soft_att = self.softmax(torch.bmm(q_s,k_s))

        self_s_att = torch.bmm(soft_att,v_s)

        tweet_level = torch.mean(self_s_att,1)
        #tweet_level = torch.mean(word_lstm,1)

        #span_out = self.sigmoid(self.span_out(self.dropout(tweet_level)))

        #comp_fc = self.relu(self.comp_fc(self.dropout(tweet_level)))

        #final_rep = torch.cat((span_out,comp_fc),1)

        #comp_out = self.comp_out(final_rep)

        #x = x.permute(0,2,1)

        #conv = self.conv(x)

        #conv = conv.permute(0,2,1)

        #conv_att = torch.mean(conv,1)

        final = self.dropout(tweet_level)

        #final = self_w_att*self.alphas_m1[0].expand_as(self_w_att) + conv_att*self.alphas_m2[0].expand_as(conv_att)
        #final = self_w_att + conv_att


        emo_fc2 = self.relu(self.emo_fc2(final))
        emo_out = self.relu(self.emo_out(emo_fc2))
        sen_fc2 = self.relu(self.sen_fc2(final))
        sen_out = self.relu(self.sen_out(sen_fc2))




       # comp_fc1 = self.relu(self.comp_fc1(x))

       # comp_fc2 = self.relu(self.comp_fc2(comp_fc1))
        comp_fc2 = self.relu(self.comp_fc2(final))
        #span_fc2 = self.relu(self.span_fc2(final))

        #central_b = emo_fc2*self.alphas_e[0].expand_as(emo_fc2) + sen_fc2*self.alphas_s[0].expand_as(sen_fc2) + comp_fc2*self.alphas_c[0].expand_as(comp_fc2)
        central_b = sen_fc2*self.alphas_s[0].expand_as(sen_fc2) + comp_fc2*self.alphas_c[0].expand_as(comp_fc2)
        #central_b = comp_fc2

        comp_fc3 = self.relu(self.comp_fc3(central_b))

        emo_cat = torch.cat((emo_out,torch.zeros(batch,121).to(device)),1)
        sen_cat = torch.cat((sen_out,torch.zeros(batch,121).to(device)),1)

        central_s = emo_cat*self.alphas_e[1].expand_as(emo_cat) + sen_cat*self.alphas_s[1].expand_as(sen_cat) + comp_fc3*self.alphas_c[1].expand_as(comp_fc3)
        #central_s = sen_cat*self.alphas_s[1].expand_as(sen_cat) + comp_fc3*self.alphas_c[1].expand_as(comp_fc3)
        #central_s = comp_fc3


        #rep = comp_fc2 + span_fc2

        #span_out = self.sigmoid(self.span_out(rep))
        span_out = self.relu(self.span_out(central_s))

        #comp_out = self.relu(self.comp_out(comp_fc2))
        comp_out = self.relu(self.comp_out(central_s))

        sev_out = self.relu(self.sev_out(central_s))




        return emo_out,sen_out,sev_out,span_out,comp_out
        #return sen_out,sev_out,span_out,comp_out
        #return sev_out,span_out,comp_out



        #return span_out,comp_out

import torch
import torchvision
import torch.nn as nn
import torch.nn.functional as F
from torch.utils.data import TensorDataset, DataLoader

from sklearn.metrics import hamming_loss
from sklearn.metrics import accuracy_score
from sklearn.metrics import confusion_matrix
from sklearn.metrics import precision_score
from sklearn.metrics import recall_score
from sklearn.metrics import f1_score
from sklearn.metrics import roc_auc_score
from sklearn.metrics import classification_report
from sklearn.metrics import  mean_absolute_error
from sklearn.model_selection import train_test_split, StratifiedKFold
from pathlib import Path
import numpy as np
import pandas as pd
import tensorflow as tf
from tensorflow import keras
import statistics


import os
import pickle


# import EarlyStopping
import os, sys
#sys.path.append('path_to_the_module/early-stopping-pytorch')
#from torchsample.callbacks import EarlyStopping


device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
#device='cpu'
print(device)




output=open("/content/drive/MyDrive/dataset-complain.pkl","rb")

vector = pickle.load(output)




task = vector["task-label"]

mean_acc=[]
mean_prec=[]
mean_recall=[]
mean_f1=[]


yes=[]
no=[]

for i in range(task.shape[0]):
    if(task[i]==1):
        yes.append(i)

    else:
        no.append(i)



print(len(yes))
print(len(no))

yes = np.array(yes)
no = np.array(no)



id = vector["id"]
tweet = vector["tweet"]
feat = vector["feat"]
emoti = vector["emoti-label"]
senti = vector["senti-label"]

task = vector["task-label"]
severity = vector["severity"]
span = vector["span"]


cnt=0



skfolds = StratifiedKFold(n_splits=5,shuffle=True)
skfolds_val = StratifiedKFold(n_splits=10,shuffle=True)


model = bert_span().to(device)
#model = conv_model().to(device)
#model = lstm(dropout, emb_size,128, embeddings).to(device)

lr = 0.00001

combined_params =  list(model.parameters())

optimizer = torch.optim.Adam(combined_params, lr=lr, weight_decay=0)

criterion_bce = nn.BCELoss() #Binary case
criterion_loss = nn.CrossEntropyLoss()




for train_index, test_index in skfolds.split(feat,task):

    feat_train = feat[train_index]
    tweet_train = tweet[train_index]
    id_train = id[train_index]
    emoti_train = emoti[train_index]
    senti_train = senti[train_index]
    #domain_train = domain[train_index]
    task_train = task[train_index]
    severity_train = severity[train_index]
    span_train = span[train_index]



    feat_test = feat[test_index]
    tweet_test = tweet[test_index]
    id_test = id[test_index]
    emoti_test = emoti[test_index]
    senti_test = senti[test_index]
    # domain_test = domain[test_index]
    task_test = task[test_index]
    severity_test = severity[test_index]
    span_test = span[test_index]
    x=feat_train
    y=task_train

    for train_ind,val_index in skfolds_val.split(x,y):



        feat_val = feat_train[val_index]
        tweet_val = tweet_train[val_index]
        id_val = id_train[val_index]
        emoti_val = emoti_train[val_index]
        senti_val = senti_train[val_index]
        #domain_val = domain_train[val_index]
        task_val = task_train[val_index]
        severity_val = severity_train[val_index]
        span_val = span_train[val_index]

        feat_train_x = feat_train[train_ind]
        tweet_train_x = tweet_train[train_ind]
        id_train_x = id_train[train_ind]
        emoti_train_x = emoti_train[train_ind]
        senti_train_x = senti_train[train_ind]
        #domain_train_x = domain_train[train_ind]
        task_train_x = task_train[train_ind]
        severity_train_x = severity_train[train_ind]
        span_train_x = span_train[train_ind]




        #tweet_train,tweet_val,task_train,task_val=train_test_split(tweet_train,y,test_size=0.1,stratify=y,random_state=0)
        #id_train,id_val,task_train,task_val=train_test_split(id_train,y,test_size=0.1,stratify=y,random_state=0)
        #feat_train,feat_val,task_train,task_val=train_test_split(feat_train,y,test_size=0.1,stratify=y,random_state=0)
        #emoti_train,emoti_val,task_train,task_val=train_test_split(emoti_train,y,test_size=0.1,stratify=y,random_state=0)
        #senti_train,senti_val,task_train,task_val=train_test_split(senti_train,y,test_size=0.1,stratify=y,random_state=0)
        #domain_train,domain_val,task_train,task_test=train_test_split(domain_train,y,test_size=0.1,stratify=y,random_state=0)
        #severity_train,severity_val,task_train,task_test=train_test_split(severity_train,y,test_size=0.1,stratify=y,random_state=0)
        #span_train,span_val,task_train,task_val=train_test_split(span_train,y,test_size=0.1,stratify=y,random_state=0)


        train={"feat":feat_train_x,"tweet":tweet_train_x,"id":id_train_x,"label":task_train_x,"emotion":emoti_train_x,"sentiment":senti_train_x,"severity":severity_train_x,"span":span_train_x}
        val={"feat":feat_val,"tweet":tweet_val,"id":id_val,"label":task_val,"emotion":emoti_val,"sentiment":senti_val,"severity":severity_val,"span":span_val}
        test={"feat":feat_test,"tweet":tweet_test,"id":id_test,"label": task_test,"emotion":emoti_test,"sentiment":senti_test,"severity":severity_test,"span":span_test}
        #test = val

        #for i in range(test["id"].shape[0]):
        #    print(test["id"][i])




    class Tweet(torch.utils.data.Dataset):
        def __init__(self,data):
            self.data=data

        def __len__(self):
            return self.data["label"].shape[0]
        def __getitem__(self,idx):

            if(torch.is_tensor(idx)):
                idx=idx.tolist()



            feat=torch.tensor(self.data["feat"][idx].astype(np.float)).float().to(device)
            id=torch.tensor(self.data["id"][idx].astype(np.float)).long().to(device)


            label=torch.tensor(self.data["label"][idx]).long().to(device)
            emotion=torch.tensor(self.data["emotion"][idx]).long().to(device)
            severity=torch.tensor(self.data["severity"][idx]).long().to(device)
            # domain=torch.tensor(self.data["domain"][idx]).long().to(device)
            sentiment=torch.tensor(self.data["sentiment"][idx]).long().to(device)
            span=torch.tensor(self.data["span"][idx]).float().to(device)


            sample = {

                "id":id,
                "feat":feat,
                "label":label,
                "severity":severity,
                # "domain":domain,
                "emotion":emotion,
                "sentiment":sentiment,
                "span":span,
            }

            return sample






    tweet_train = Tweet(train)
    dataloader_train = DataLoader(tweet_train, batch_size=32,shuffle=False, num_workers=0)

    print("train_data loaded")

    tweet_val = Tweet(val)
    dataloader_val = DataLoader(tweet_val, batch_size=32,shuffle=False, num_workers=0)
    print("validation_data loaded")


    tweet_test = Tweet(test)
    dataloader_test = DataLoader(tweet_test, batch_size=32,shuffle=False, num_workers=0)

model = torch.load("/content/drive/MyDrive/My_model1.pt")
model.eval()

def test_model(model, abc):
    model.eval()


    total_test = 0
    correct_test =0
    total_acc_test = 0
    total_loss_test = 0
    outputs = []

    test_labels=[]
    with torch.no_grad():
        for data in dataloader_test:


            feat = data['feat'].to(device)

            label_test = data['label'].to(device)
            span_test = data['span'].to(device)
            emotion_test = data["emotion"].to(device)
            sentiment_test = data['sentiment'].to(device)
            severity_test = data['severity'].to(device)


            #_,out = model(doc)
            emo_out,sen_out,sev_out,span_out,comp_out= model(feat)

            g = [emo_out,sen_out,sev_out,span_out,comp_out]
            f = {'emo_out': emotion_test.squeeze(1).long(),'sen_out':sentiment_test.squeeze(1).long(),'sev_out': severity_test.squeeze(1).long(),'span_out': span_test,'comp_out': label_test.squeeze(1).long()}
            i = abc
            j = f[abc]



            outputs += list(span_out.cpu().data.numpy())
            loss = criterion_loss(g[4].data,j)
            #loss = criterion_loss(out, label_test.long())

            _, predicted_test = torch.max(g[4].data, 1)
            #predicted_test = (out.data>=0.5).float()
            total_test += label_test.size(0)
            #_, label_test = torch.max(label_test, 1)
            correct_test += (predicted_test == j).sum().item()
            #correct_test += (predicted_test == label_test).sum().item()
#                 out_val = (output.squeeze()>0.5).float()
#                 out_final = ((out_val == 1).nonzero(as_tuple=True)[0])
#                 print()
#                 acc = torch.abs(output.squeeze() - label.float()).view(-1)
#                 acc = (1. - acc.sum() / acc.size()[0])
#                 total_acc_train += acc
            total_loss_test += loss.item()


#     #         print(label.float())
#             acc = torch.abs(out.squeeze() - label.float()).view(-1)
#     #         print((acc.sum() / acc.size()[0]))
#             acc = (1. - acc.sum() / acc.size()[0])
#     #         print(acc)
#             total_acc_test += acc
#             total_loss_test += loss.item()


    acc_test = 100 * correct_test / total_test
    loss_test = total_loss_test/total_test

    print(f'acc: {acc_test:.4f} loss: {loss_test:.4f}')
    return outputs



    #chk_file = os.path.join(exp_path, 'checkpoint_'+exp_name+'.pt')
    #model.load_state_dict(torch.load(chk_file))
    # Plot the training and validation curves


outputs = test_model(model, 'comp_out')

tweet=test["tweet"]
print(tweet.shape)
true=test["span"]

print(true.shape[0])
print(true.shape[1])
print(len(outputs))



explain_true=[]
explain_pred=[]


for i in range(len(true)):
    pred=[]
    truth=[]
    for j in range(len(true[i])):
        if(true[i][j]>=0.5):
            if tweet[i][j] == ' ':
                continue
            truth.append(tweet[i][j])

        if(outputs[i][j]>0):
            if tweet[i][j] == ' ':
                continue
            pred.append(tweet[i][j])
            #print(tweet[i][j])




    if(len(truth)==0):
        truth.append('tok')

    if(len(pred)==0):
        pred.append('tok')

    #print(pred)
    explain_true.append(truth)
    explain_pred.append(pred)





cnt=0
IOUlist=0
IOULIST=[]

pred=[]
truth=[]

for i in range(len(explain_true)):
    a=explain_true[i]
    b=explain_pred[i]

    truth.append(len(a))
    pred.append(len(b))

    intersection=list(set(a) & set(b))
    uniuon=list(set(a) | set(b))
    IOU=len(intersection)/len(uniuon)
    if IOU>=0.5:
        IOU=1
    else:
        IOU=0
    IOULIST.append(IOU)
    IOUlist+=IOU
    #maxlen=max(maxlen,len(predicted))
    cnt=cnt+1






print( "Jaccard distance ",IOUlist/cnt)
recall=sum(IOULIST)/sum(pred)
precision=sum(IOULIST)/sum(truth)

print("the recall is ",recall)
print("the precision is ",precision)
IOUF1=2*((precision*recall)/(precision+recall))
print("the IOUF1 is ",IOUF1)



d = {'explain_pred': explain_pred, 'explain_true': explain_true}
df_final = pd.DataFrame(data=d)

df_final.to_csv("explain_pred.csv")

